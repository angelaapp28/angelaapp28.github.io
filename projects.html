<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <!-- favicon -->
    <link rel="icon" type="image/png" href="favicon.png">
    <title>Projects | Angela Appiah</title>
    <style>
        /* main container */
        .projects-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* page title */
        .page-title {
            text-align: center;
            margin-bottom: 50px;
            color: #2c3e50;
            font-size: 2.5rem;
            position: relative;
        }

        .page-title::after {
            content: '';
            position: absolute;
            bottom: -15px;
            left: 50%;
            transform: translateX(-50%);
            width: 100px;
            height: 4px;
            background: #68a762;
            border-radius: 2px;
        }

        /* tabs nav */
        .tabs-container {
            margin-bottom: 40px;
            border-bottom: 2px solid #e0e0e0;
            overflow-x: auto;
            white-space: nowrap;
            scrollbar-width: none;
            -ms-overflow-style: none;
        }

        .tabs-container::-webkit-scrollbar {
            display: none;
        }

        .tabs {
            display: inline-flex;
            min-width: 100%;
        }

        .tab {
            padding: 15px 25px;
            background: none;
            border: none;
            cursor: pointer;
            font-size: 1.1em;
            color: #666;
            position: relative;
            transition: all 0.3s ease;
            white-space: nowrap;
            border-bottom: 3px solid transparent;
            margin-bottom: -2px;
        }

        .tab:hover {
            color: #68a762;
            background: #f8f9fa;
        }

        .tab.active {
            color: #68a762;
            font-weight: 600;
            border-bottom: 3px solid #68a762;
        }

        /* tab content */
        .tab-content {
            display: none;
            animation: fadeIn 0.5s ease-in-out;
            padding: 30px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08);
            margin-bottom: 40px;
        }

        .tab-content.active {
            display: block;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* proj header */
        .project-header {
            margin-bottom: 30px;
            border-bottom: 1px solid #eee;
            padding-bottom: 20px;
        }

        .project-header h2 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2rem;
        }

        .project-tagline {
            color: #68a762;
            font-size: 1.2rem;
            font-weight: 500;
        }

        /* proj content */
        .project-overview {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 40px;
            margin-bottom: 40px;
        }

        @media (max-width: 768px) {
            .project-overview {
                grid-template-columns: 1fr;
                gap: 30px;
            }
        }

        /* proj details */
        .project-details h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.5rem;
        }

        .project-details p {
            line-height: 1.7;
            color: #555;
            margin-bottom: 20px;
        }

        .project-details ul {
            color: #555;
            line-height: 1.7;
            margin-bottom: 20px;
            padding-left: 20px;
        }

        .project-details li {
            margin-bottom: 8px;
        }

        /* math notation */
        .math-formula {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #68a762;
        }

        .math-formula p {
            margin: 10px 0;
            font-family: 'Cambria Math', 'Times New Roman', serif;
            font-size: 1.1em;
        }

        /* tech sidebar */
        .project-technologies {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 25px;
            border-radius: 10px;
            border-left: 4px solid #68a762;
            height: fit-content;
        }

        .project-technologies h4 {
            margin-top: 0;
            color: #68a762;
            font-size: 1.3rem;
            margin-bottom: 15px;
        }

        .tech-list {
            list-style: none;
            padding: 0;
        }

        .tech-list li {
            padding: 8px 0;
            border-bottom: 1px dashed #ddd;
            color: #555;
        }

        .tech-list li:last-child {
            border-bottom: none;
        }

        /* code highlight */
        .code-highlight {
            background: #2d3748;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 10px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            margin: 20px 0;
            border: 1px solid #4a5568;
        }

        .code-highlight pre {
            margin: 0;
        }

        .code-highlight code {
            line-height: 1.6;
        }

        .code-highlight .python {
            color: #cbd5e0;
        }

        .code-highlight .keyword {
            color: #63b3ed;
        }

        .code-highlight .string {
            color: #68d391;
        }

        .code-highlight .comment {
            color: #a0aec0;
            font-style: italic;
        }

 

        /* Features List */
        .features-list {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .feature {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 3px solid #68a762;
        }

        .feature h4 {
            color: #2c3e50;
            margin-bottom: 10px;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .projects-container {
                padding: 20px 15px;
            }

            .page-title {
                font-size: 2rem;
                margin-bottom: 30px;
            }

            .tab {
                padding: 12px 20px;
                font-size: 1em;
            }

            .tab-content {
                padding: 20px;
            }

            .project-header h2 {
                font-size: 1.8rem;
            }

            
        }
    </style>
</head>

<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="resume.html">Resume</a>
            <a href="projects.html">Projects</a>
            <a href="contact.html">Contact</a>
        </nav>
    </header>

    <div class="projects-container">
        <h1 class="page-title">Projects Portfolio</h1>

        <div class="tabs-container">
            <div class="tabs">
                <button class="tab active" onclick="openTab('tab1')">Surgical Navigation</button>
                <button class="tab" onclick="openTab('tab2')">AI Language Model</button>
                <button class="tab" onclick="openTab('tab3')">Robot Navigation</button>
                <button class="tab" onclick="openTab('tab4')">Surgical Image Seg.</button>
                <button class="tab" onclick="openTab('tab5')">Android Mobile Game</button>
            </div>
        </div>

        <!-- Project 1: Surgical Navigation Registration -->
        <div id="tab1" class="tab-content active">
            <div class="project-header">
                <h2>Surgical Navigation Registration with ICP</h2>
                <p class="project-tagline">3D point cloud registration for CT-based surgical navigation using iterative closest point algorithm</p>
            </div>

            <div class="project-overview">
                <div class="project-details">
                    <h3>Project Overview</h3>
                    <p>This project extends a surgical navigation system by implementing the full Iterative Closest Point (ICP) algorithm for aligning 3D medical scans. While previous implementations used identity transformations, this complete ICP solution iteratively estimates the optimal registration transformation \(F_{reg}\) that aligns pointer tip positions with bone surface meshes from pre-operative CT data. The system enables precise registration between physical space and medical imaging data, essential for accurate surgical navigation.</p>
                    
                    <p>The ICP algorithm alternates between finding closest point correspondences and computing optimal rigid transformations until convergence, significantly improving registration accuracy compared to single-pass approaches.</p>

                    <h3>Mathematical Formulation</h3>
                    <div class="math-formula">
                        <p><strong>ICP Error Minimization:</strong></p>
                        <p>\[E(F_{reg}) = \sum_{k=1}^{N}\left\| F_{reg}\cdot d_{k} - c_{k}\right\|^{2}\]</p>
                        <p>where:</p>
                        <p>\(d_{k} = F_{B,k}^{-1}\cdot F_{A,k}\cdot A_{tip}\)</p>
                        <p>\(c_{k} =\) closest point on mesh to \(F_{reg}\cdot d_{k}\)</p>
                        <p>\(F_{reg} =\) registration transformation from B coordinates to CT coordinates</p>
                    </div>

                    <h3>Core Algorithm Implementation</h3>
                    <div class="code-highlight">
                        <pre><code class="python">
<span class="keyword">def</span> <span class="function">icp_registration</span>(all_d_k, vertices, triangles, max_iterations=50, tolerance=1e-6):
    <span class="comment"># Initialize transformation matrix (PA3 solution)</span>
    F_reg = np.eye(4)
    prev_error = float('inf')
    
    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(max_iterations):
        transformed_points = []
        closest_points = []
        distances = []
        
        <span class="keyword">for</span> d_k <span class="keyword">in</span> all_d_k:
            <span class="comment"># Transform points using current registration</span>
            s_k = F_reg @ np.append(d_k, 1.0)
            
            <span class="comment"># Find closest points on mesh</span>
            c_k, dist = closest_point_mesh(s_k[:3], vertices, triangles)
            
            transformed_points.append(s_k[:3])
            closest_points.append(c_k)
            distances.append(dist)
        
        current_error = np.mean(distances)
        
        <span class="comment"># Compute new transformation using Kabsch algorithm</span>
        F_reg_new = compute_optimal_transform(all_d_k, closest_points)
        
        <span class="comment"># Check convergence criteria</span>
        transform_change = np.linalg.norm(F_reg_new - F_reg)
        <span class="keyword">if</span> transform_change < tolerance:
            <span class="keyword">break</span>
        
        F_reg = F_reg_new.copy()
    
    <span class="keyword">return</span> F_reg, transformed_points, closest_points, distances
                        </code></pre>
                    </div>

                    <div class="features-list">
                        <div class="feature">
                            <h4>Medical Precision</h4>
                            <p>Sub-millimeter accuracy suitable for surgical applications with consistent low error rates across all test cases</p>
                        </div>
                        <div class="feature">
                            <h4>Iterative Refinement</h4>
                            <p>Full ICP implementation with convergence monitoring and adaptive stopping criteria</p>
                        </div>
                        <div class="feature">
                            <h4>Modular Architecture</h4>
                            <p>Reusable components from PA3 extended for enhanced functionality and maintainability</p>
                        </div>
                    </div>
                </div>

                <div class="project-technologies">
                    <h4>Core Technologies</h4>
                    <ul class="tech-list">
                        <li>Python 3.x</li>
                        <li>NumPy (Linear Algebra)</li>
                        <li>Iterative Closest Point Algorithm</li>
                        <li>Kabsch Algorithm</li>
                        <li>Rigid Body Transformations</li>
                        <li>3D Geometry Computations</li>
                    </ul>

                    <h4 style="margin-top: 30px;">Key Algorithms</h4>
                    <ul class="tech-list">
                        <li>Iterative Closest Point (ICP)</li>
                        <li>Kabsch Algorithm for SVD-based registration</li>
                        <li>Point-to-Mesh Distance Computation</li>
                        <li>Convergence Detection</li>
                        <li>Error Metric Calculation</li>
                    </ul>

                    <h4 style="margin-top: 30px;">Validation</h4>
                    <ul class="tech-list">
                        <li>Unit Testing Framework</li>
                        <li>Debug Dataset Validation</li>
                        <li>Error Metric Analysis</li>
                        <li>Numerical Precision Verification</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Project 2: AI Language Model -->
        <div id="tab2" class="tab-content">
            <div class="project-header">
                <h2>Decoder-Only Transformer Language Model (TinyGPT)</h2>
                <p class="project-tagline">Building and training a character-level GPT model from scratch with self-attention mechanisms</p>
            </div>
            <div class="project-overview">
                <div class="project-details">
                    <h3>Project Overview</h3>
                    <p>Implemented a decoder-only Transformer architecture (TinyGPT) for character-level language modeling, trained on the Tiny Shakespeare dataset. The model features multi-head self-attention with causal masking, feed-forward networks with GELU activations, and positional embeddings. The implementation includes the complete training pipeline with AdamW optimization and gradient checkpointing for memory efficiency.</p>
                    
                    <h3>Core Architecture</h3>
                    <p>The model implements the standard Transformer decoder architecture with the following components:</p>
                    <ul>
                        <li><strong>Self-Attention Head</strong>: Implements Q/K/V linear projections with causal masking</li>
                        <li><strong>Multi-Head Attention</strong>: Parallel attention heads with output projection</li>
                        <li><strong>Transformer Block</strong>: Pre-layer normalization with residual connections</li>
                        <li><strong>Feed-Forward Network</strong>: Two-layer MLP with GELU activation</li>
                    </ul>
                    
                    <div class="code-highlight">
                        <pre><code class="python">
<span class="keyword">class</span> <span class="function">SelfAttentionHead</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, head_size, embed_dim, block_size, dropout=0.0):
        <span class="keyword">super</span>().__init__()
        self.key = nn.Linear(embed_dim, head_size, bias=False)
        self.query = nn.Linear(embed_dim, head_size, bias=False)
        self.value = nn.Linear(embed_dim, head_size, bias=False)
        self.attn_drop = nn.Dropout(dropout)
        self.resid_drop = nn.Dropout(dropout)
        
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)
        
        attn_scores = q @ k.transpose(-2, -1) / (k.size(-1) ** 0.5)
        attn_scores = attn_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))
        attn_prob = F.softmax(attn_scores, dim=-1)
        out = attn_prob @ v
        <span class="keyword">return</span> out
                        </code></pre>
                    </div>
                    
                    <h3>Fine-Tuning GPT-2</h3>
                    <p>Extended the project by fine-tuning a pre-trained GPT-2 model on custom text corpora. Implemented text preprocessing, dataset creation, and training loop with linear schedule warmup. Achieved coherent text generation with temperature sampling and top-k filtering.</p>
                    
                    <div class="features-list">
                        <div class="feature">
                            <h4>From-Scratch Implementation</h4>
                            <p>Built Transformer architecture from first principles without relying on high-level libraries</p>
                        </div>
                        <div class="feature">
                            <h4>Character-Level Modeling</h4>
                            <p>Trained on raw character sequences with custom tokenizer implementation</p>
                        </div>
                        <div class="feature">
                            <h4>Production-Grade Fine-Tuning</h4>
                            <p>Leveraged Hugging Face transformers for efficient GPT-2 fine-tuning</p>
                        </div>
                    </div>
                </div>
                
                <div class="project-technologies">
                    <h4>Core Technologies</h4>
                    <ul class="tech-list">
                        <li>PyTorch 2.0+</li>
                        <li>Transformers Architecture</li>
                        <li>Multi-Head Self-Attention</li>
                        <li>Hugging Face Transformers</li>
                        <li>AdamW Optimizer</li>
                        <li>GELU Activation</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Model Specifications</h4>
                    <ul class="tech-list">
                        <li>Embedding Dimension: 192</li>
                        <li>Number of Layers: 4</li>
                        <li>Attention Heads: 4</li>
                        <li>Block Size: 128 tokens</li>
                        <li>Vocabulary Size: 65 (characters)</li>
                        <li>Dropout: 0.0 (for TinyGPT)</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Training Details</h4>
                    <ul class="tech-list">
                        <li>Dataset: Tiny Shakespeare</li>
                        <li>Learning Rate: 3e-4</li>
                        <li>Batch Size: 64</li>
                        <li>Training Steps: 2000</li>
                        <li>Loss Function: Cross-Entropy</li>
                        <li>GPU Acceleration: CUDA/T4</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Project 3: Robot Navigation -->
        <div id="tab3" class="tab-content">
            <div class="project-header">
                <h2>Probabilistic Robot Navigation with Beam Model</h2>
                <p class="project-tagline">SLAM-based navigation with particle filtering and sensor fusion</p>
            </div>
            <div class="project-overview">
                <div class="project-details">
                    <h3>Project Overview</h3>
                    <p>Implemented a complete probabilistic robot navigation system featuring beam range finder models and odometry motion models for particle filtering-based SLAM. The system computes the probability P(z|s,m) of laser measurements given robot state and map, incorporating four error models: correct range measurements, unexpected objects, sensor failures, and random noise.</p>
                    
                    <h3>Beam Range Finder Model</h3>
                    <p>The beam model combines four probability distributions to handle different measurement scenarios:</p>
                    
                    <div class="math-formula">
                        <p><strong>Total Probability:</strong> p = w_hit·p_hit + w_short·p_short + w_max·p_max + w_rand·p_rand</p>
                        <p><strong>Correct Measurement:</strong> p_hit = η·N(r; r_s, σ_hit²)</p>
                        <p><strong>Unexpected Object:</strong> p_short = η·λ_short·exp(-λ_short·r)</p>
                        <p><strong>Sensor Failure:</strong> p_max = I(r = z_max)</p>
                        <p><strong>Random Noise:</strong> p_rand = Uniform(0, z_max)</p>
                    </div>
                    
                    <div class="code-highlight">
                        <pre><code class="cpp">
<span class="keyword">double</span> <span class="function">beam_model</span>(Map* map, Scan& z, State s, Pose laser_pose,
                <span class="keyword">double</span> sigma_hit, <span class="keyword">double</span> lambda_short, <span class="keyword">double</span> z_max,
                <span class="keyword">double</span> w_hit, <span class="keyword">double</span> w_short, <span class="keyword">double</span> w_max, <span class="keyword">double</span> w_rand) {
    
    <span class="keyword">double</span> p = 1.0;
    <span class="comment">// Transform laser to world coordinates</span>
    <span class="keyword">double</span> laser_world_x = robot_x + (laser_x * cos(robot_theta)) - (laser_y * sin(robot_theta));
    <span class="keyword">double</span> laser_world_y = robot_y + (laser_x * sin(robot_theta)) + (laser_y * cos(robot_theta));
    
    <span class="keyword">for</span> (size_t i = 0; i < z.size(); i++) {
        <span class="comment">// Compute expected range from map</span>
        <span class="keyword">double</span> expected_range = map_calc_range(map, laser_world_x, laser_world_y, 
                                                beam_world_bearing, z_max);
        
        <span class="comment">// Combine probability models</span>
        <span class="keyword">double</span> phit = p_hit(measured_range, expected_range, z_max, sigma_hit);
        <span class="keyword">double</span> pshort = p_short(measured_range, expected_range, z_max, lambda_short);
        <span class="keyword">double</span> pmax = p_max(measured_range, expected_range, z_max);
        <span class="keyword">double</span> prand = p_rand(measured_range, expected_range, z_max);
        
        <span class="keyword">double</span> pz = w_hit*phit + w_short*pshort + w_max*pmax + w_rand*prand;
        p += log(pz + 1e-9); <span class="comment">// Log probabilities for numerical stability</span>
    }
    <span class="keyword">return</span> p;
}
                        </code></pre>
                    </div>
                    
                    <h3>Odometry Motion Model</h3>
                    <p>Implemented sample-based odometry model with error parameters (α1-α4) for rotational and translational noise:</p>
                    
                    <div class="code-highlight">
                        <pre><code class="cpp">
<span class="keyword">void</span> <span class="function">odometry_model</span>(<span class="keyword">double</span> sample[3], <span class="keyword">double</span> delta[3],
                    <span class="keyword">double</span> alpha1, <span class="keyword">double</span> alpha2, <span class="keyword">double</span> alpha3, <span class="keyword">double</span> alpha4) {
    
    <span class="comment">// Compute motion components</span>
    <span class="keyword">double</span> delta_rot1 = atan2(delta_y, delta_x) - theta;
    <span class="keyword">double</span> delta_trans = sqrt(delta_x * delta_x + delta_y * delta_y);
    <span class="keyword">double</span> delta_rot2 = delta_theta - delta_rot1;
    
    <span class="comment">// Add Gaussian noise based on motion parameters</span>
    <span class="keyword">double</span> delta_rot1_hat = delta_rot1 - pf_ran_gaussian(alpha1*pow(delta_rot1,2) + alpha2*pow(delta_trans,2));
    <span class="keyword">double</span> delta_trans_hat = delta_trans - pf_ran_gaussian(alpha3*pow(delta_trans,2) + alpha4*pow(delta_rot1,2) + alpha4*pow(delta_rot2,2));
    <span class="keyword">double</span> delta_rot2_hat = delta_rot2 - pf_ran_gaussian(alpha1*pow(delta_rot2,2) + alpha2*pow(delta_trans,2));
    
    <span class="comment">// Update sample pose</span>
    sample[0] = x + delta_trans_hat * cos(theta + delta_rot1_hat);
    sample[1] = y + delta_trans_hat * sin(theta + delta_rot1_hat);
    sample[2] = theta + delta_rot1_hat + delta_rot2_hat;
}
                        </code></pre>
                    </div>
                    
                    <div class="features-list">
                        <div class="feature">
                            <h4>Probabilistic Sensor Fusion</h4>
                            <p>Combines multiple error models for robust range finder measurements</p>
                        </div>
                        <div class="feature">
                            <h4>Sample-Based Localization</h4>
                            <p>Uses particle filtering with odometry and sensor updates</p>
                        </div>
                        <div class="feature">
                            <h4>Map-Consistent Navigation</h4>
                            <p>Integrates with occupancy grid maps for accurate pose estimation</p>
                        </div>
                    </div>
                </div>
                
                <div class="project-technologies">
                    <h4>Core Algorithms</h4>
                    <ul class="tech-list">
                        <li>Beam Range Finder Model</li>
                        <li>Odometry Motion Model</li>
                        <li>Particle Filter (Monte Carlo)</li>
                        <li>Probabilistic Robotics</li>
                        <li>Sensor Fusion</li>
                        <li>Occupancy Grid Mapping</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Mathematical Models</h4>
                    <ul class="tech-list">
                        <li>Gaussian Distribution (p_hit)</li>
                        <li>Exponential Distribution (p_short)</li>
                        <li>Uniform Distribution (p_rand)</li>
                        <li>Delta Function (p_max)</li>
                        <li>Error Function Normalization</li>
                        <li>Log-Probabilities for Stability</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Implementation Details</h4>
                    <ul class="tech-list">
                        <li>Language: C++</li>
                        <li>Coordinate Transformations</li>
                        <li>Numerical Stability Handling</li>
                        <li>Boundary Condition Checks</li>
                        <li>Angle Normalization</li>
                        <li>Error Parameter Tuning</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Project 4: Surgical Image Segmentation -->
        <div id="tab4" class="tab-content">
            <div class="project-header">
                <h2>Surgical Instrument Segmentation with U-Net</h2>
                <p class="project-tagline">Real-time semantic segmentation of surgical tools in endoscopic videos</p>
            </div>
            <div class="project-overview">
                <div class="project-details">
                    <h3>Project Overview</h3>
                    <p>Developed a real-time semantic segmentation system for surgical guidance using U-Net architecture to identify and delineate surgical instruments in endoscopic video streams. The system provides pixel-wise classification to enable augmented reality overlays, surgical guidance visualization, and instrument tracking during minimally invasive procedures.</p>
                    
                    <h3>Architecture Design</h3>
                    <p>The system implements a U-Net based architecture with encoder-decoder structure:</p>
                    <ul>
                        <li><strong>Encoder Path</strong>: Feature extraction with down-sampling through convolutional blocks</li>
                        <li><strong>Bottleneck</strong>: High-level feature representation at lowest resolution</li>
                        <li><strong>Decoder Path</strong>: Feature up-sampling with skip connections from encoder</li>
                        <li><strong>Skip Connections</strong>: Preserve spatial information through concatenation</li>
                    </ul>
                    
                    <h3>Key Features</h3>
                    <div class="features-list">
                        <div class="feature">
                            <h4>Real-Time Processing</h4>
                            <p>Optimized for >30 FPS inference on endoscopic video streams</p>
                        </div>
                        <div class="feature">
                            <h4>Multi-Class Segmentation</h4>
                            <p>Simultaneous identification of multiple surgical instrument types</p>
                        </div>
                        <div class="feature">
                            <h4>Augmented Reality Integration</h4>
                            <p>Output compatible with surgical AR overlay systems</p>
                        </div>
                    </div>
                </div>
                
                <div class="project-technologies">
                    <h4>Core Technologies</h4>
                    <ul class="tech-list">
                        <li>U-Net Architecture</li>
                        <li>PyTorch / TensorFlow</li>
                        <li>OpenCV</li>
                        <li>TensorRT Optimization</li>
                        <li>Medical Imaging Libraries</li>
                        <li>CUDA Acceleration</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Model Architecture</h4>
                    <ul class="tech-list">
                        <li>Encoder Depth: 4-5 levels</li>
                        <li>Skip Connections: Concatenation</li>
                        <li>Loss Function: Dice + Cross-Entropy</li>
                        <li>Optimizer: Adam</li>
                        <li>Input Resolution: 512x512</li>
                        <li>Output Classes: Multiple instrument types</li>
                    </ul>
                    
                    <h4 style="margin-top: 30px;">Data Processing</h4>
                    <ul class="tech-list">
                        <li>Endoscopic Video Frames</li>
                        <li>Data Augmentation</li>
                        <li>Pixel-wise Annotation</li>
                        <li>Real-time Inference</li>
                        <li>Post-processing</li>
                        <li>Visualization Pipeline</li>
                    </ul>
                </div>
            </div>
        </div>
        
    </div>

    <script>
        function openTab(tabName) {
            var tabContents = document.getElementsByClassName("tab-content");
            for (var i = 0; i < tabContents.length; i++) {
                tabContents[i].classList.remove("active");
            }

            var tabs = document.getElementsByClassName("tab");
            for (var i = 0; i < tabs.length; i++) {
                tabs[i].classList.remove("active");
            }

            document.getElementById(tabName).classList.add("active");
            event.currentTarget.classList.add("active");

            if (window.innerWidth < 768) {
                document.getElementById(tabName).scrollIntoView({
                    behavior: 'smooth',
                    block: 'start'
                });
            }
        }

        window.addEventListener('resize', function () {
            if (window.innerWidth < 768) {
                const activeTab = document.querySelector('.tab.active');
                if (activeTab) {
                    activeTab.scrollIntoView({
                        behavior: 'smooth',
                        inline: 'center',
                        block: 'nearest'
                    });
                }
            }
        });
    </script>

</body>

</html>